apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: health-data-consumer
  namespace: data-platform
  labels:
    app: flink-iceberg-consumer
    version: "1.0.0"
spec:
  image: health-stack/flink-iceberg-consumer:1.0.0
  imagePullPolicy: IfNotPresent
  flinkVersion: v1_18
  
  # Flink configuration
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "4"
    state.backend: rocksdb
    state.backend.incremental: "true"
    state.checkpoints.dir: s3a://flink-checkpoints/health-consumer
    state.savepoints.dir: s3a://flink-checkpoints/health-consumer/savepoints
    execution.checkpointing.interval: 60s
    execution.checkpointing.mode: EXACTLY_ONCE
    execution.checkpointing.timeout: 10min
    execution.checkpointing.min-pause: 30s
    execution.checkpointing.max-concurrent-checkpoints: "1"
    restart-strategy: fixed-delay
    restart-strategy.fixed-delay.attempts: "3"
    restart-strategy.fixed-delay.delay: 10s
    metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter
    metrics.reporter.prom.port: "9249"
  
  # Service account
  serviceAccount: flink-service-account
  
  # JobManager configuration
  jobManager:
    resource:
      memory: "2048m"
      cpu: 1
    replicas: 1
    podTemplate:
      spec:
        containers:
          - name: flink-main-container
            env:
              - name: KAFKA_BROKERS
                value: "kafka-broker-1.kafka.svc.cluster.local:9092,kafka-broker-2.kafka.svc.cluster.local:9092,kafka-broker-3.kafka.svc.cluster.local:9092"
              - name: KAFKA_TOPIC
                value: "health-data-raw"
              - name: KAFKA_GROUP_ID
                value: "flink-iceberg-consumer"
              - name: SCHEMA_REGISTRY_URL
                value: "http://schema-registry.kafka.svc.cluster.local:8081"
              - name: ICEBERG_CATALOG_TYPE
                value: "hadoop"
              - name: ICEBERG_WAREHOUSE
                value: "s3a://data-lake/warehouse"
              - name: S3_ENDPOINT
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: endpoint
              - name: S3_ACCESS_KEY
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: access-key
              - name: S3_SECRET_KEY
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: secret-key
              - name: FLINK_PARALLELISM
                value: "6"
              - name: LOG_LEVEL
                value: "INFO"
            volumeMounts:
              - name: flink-config
                mountPath: /opt/flink/conf/flink-conf.yaml
                subPath: flink-conf.yaml
              - name: flink-config
                mountPath: /opt/flink/conf/log4j2.properties
                subPath: log4j2.properties
        volumes:
          - name: flink-config
            configMap:
              name: flink-config
  
  # TaskManager configuration
  taskManager:
    resource:
      memory: "4096m"
      cpu: 2
    replicas: 3
    podTemplate:
      spec:
        containers:
          - name: flink-main-container
            env:
              - name: KAFKA_BROKERS
                value: "kafka-broker-1.kafka.svc.cluster.local:9092,kafka-broker-2.kafka.svc.cluster.local:9092,kafka-broker-3.kafka.svc.cluster.local:9092"
              - name: KAFKA_TOPIC
                value: "health-data-raw"
              - name: KAFKA_GROUP_ID
                value: "flink-iceberg-consumer"
              - name: SCHEMA_REGISTRY_URL
                value: "http://schema-registry.kafka.svc.cluster.local:8081"
              - name: ICEBERG_CATALOG_TYPE
                value: "hadoop"
              - name: ICEBERG_WAREHOUSE
                value: "s3a://data-lake/warehouse"
              - name: S3_ENDPOINT
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: endpoint
              - name: S3_ACCESS_KEY
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: access-key
              - name: S3_SECRET_KEY
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: secret-key
              - name: LOG_LEVEL
                value: "INFO"
            volumeMounts:
              - name: flink-config
                mountPath: /opt/flink/conf/flink-conf.yaml
                subPath: flink-conf.yaml
              - name: flink-config
                mountPath: /opt/flink/conf/log4j2.properties
                subPath: log4j2.properties
        volumes:
          - name: flink-config
            configMap:
              name: flink-config
  
  # Job configuration
  job:
    jarURI: local:///opt/flink/opt/flink-python_2.12-1.18.0.jar
    args:
      - "-py"
      - "/opt/flink-app/main.py"
    parallelism: 6
    upgradeMode: savepoint
    state: running
    savepointTriggerNonce: 0
  
  # Pod disruption budget
  podTemplate:
    spec:
      restartPolicy: Always
